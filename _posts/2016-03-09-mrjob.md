---
layout: post
title:  "使用python mrjob做mapreduce"
date:   2016-03-09 09:05:31
categories: python hadoop mrjob
tags: python hadoop mrjob
---

* content
{:toc}


## 安装mrjob
{% highlight python %}
pip install mrjob
{% endhighlight %}

## 第一个例子
{% highlight python %}
from mrjob.job import MRJob


class MRWordFrequencyCount(MRJob):

    def mapper(self, _, line):
        yield "chars", len(line)
        yield "words", len(line.split())
        yield "lines", 1

    def reducer(self, key, values):
        yield key, sum(values)


if __name__ == '__main__':
    MRWordFrequencyCount.run()

{% endhighlight %}

运行

{% highlight python %}
python word_count.py my_file.txt
{% endhighlight %}

结果

{% highlight python %}
"chars" 3654
"lines" 123
"words" 417
{% endhighlight %}

## 倒排索引例子

多个文件，求字符串在文件中出现次数，并从大到小排序

```python

import os

from mrjob.job import MRJob
from mrjob.step import MRStep


class InverseIndex(MRJob):

    def steps(self):
        return [
            MRStep(mapper=self.mapper_get_words,
                   combiner=self.combiner_count_words,
                   reducer=self.reducer_count_words),
            MRStep(mapper=self.get_one_file_counts,
                reducer=self.reducer2)
        ]

    def mapper1(self, _, line):
        file_name = os.environ['mapreduce_map_input_file']
        for word in line.split():
            yield (word, file_name), 1

    def combiner1(self, key, counts):
        yield key, sum(counts)

    def reducer1(self, key, counts):
        yield key, sum(counts)

    def mapper2(self, word_file_pairs, counts):
        yield word_file_pairs[0], (word_file_pairs[1], counts)

    def reducer2(self, word, file_counts):
        res = list(file_counts)
        res.sort(key=lambda x:x[1], reverse=True)
        yield word, res


if __name__ == '__main__':
    InverseIndex.run()

```

结果：

```python

"hello" [["3.txt", 6], ["2.txt", 4], ["1.txt", 3]]
"jerry" [["1.txt", 1]]
"mouren"    [["2.txt", 1], ["3.txt", 1]]
"mr"    [["3.txt", 3], ["2.txt", 1]]
"tom"   [["1.txt", 2], ["3.txt", 2], ["2.txt", 1]]
"xxx"   [["2.txt", 1]]

```

## 多个步骤的例子

{% highlight python %}
from mrjob.job import MRJob
from mrjob.step import MRStep
import re

WORD_RE = re.compile(r"[\w']+")


class MRMostUsedWord(MRJob):

    def steps(self):
        return [
            MRStep(mapper=self.mapper_get_words,
                   combiner=self.combiner_count_words,
                   reducer=self.reducer_count_words),
            MRStep(reducer=self.reducer_find_max_word)
        ]

    def mapper_get_words(self, _, line):
        # yield each word in the line
        for word in WORD_RE.findall(line):
            yield (word.lower(), 1)

    def combiner_count_words(self, word, counts):
        # optimization: sum the words we've seen so far
        yield (word, sum(counts))

    def reducer_count_words(self, word, counts):
        # send all (num_occurrences, word) pairs to the same reducer.
        # num_occurrences is so we can easily use Python's max() function.
        yield None, (sum(counts), word)

    # discard the key; it is just None
    def reducer_find_max_word(self, _, word_count_pairs):
        # each item of word_count_pairs is (count, word),
        # so yielding one results in key=counts, value=word
        yield max(word_count_pairs)


if __name__ == '__main__':
    MRMostUsedWord.run()
{% endhighlight %}

## 每个阶段都一些处理函数，我们可以重写

* mapper()
* combiner()
* reducer()
* mapper_init()
* combiner_init()
* reducer_init()
* mapper_final()
* combiner_final()
* reducer_final()
* mapper_cmd()
* combiner_cmd()
* reducer_cmd()
* mapper_pre_filter()
* combiner_pre_filter()
* reducer_pre_filter()

mapper开始前的处理和结束后的处理

```python
from mrjob.job import MRJob
from mrjob.step import MRStep

class MRWordFreqCount(MRJob):

    def init_get_words(self):
        self.words = {}

    def get_words(self, _, line):
        for word in WORD_RE.findall(line):
            word = word.lower()
            self.words.setdefault(word, 0)
            self.words[word] = self.words[word] + 1

    def final_get_words(self):
        for word, val in self.words.iteritems():
            yield word, val

    def sum_words(self, word, counts):
        yield word, sum(counts)

    def steps(self):
        return [MRStep(mapper_init=self.init_get_words,
                       mapper=self.get_words,
                       mapper_final=self.final_get_words,
                       combiner=self.sum_words,
                       reducer=self.sum_words)]
```

如果定了*_cmd() 方法，则会使用shell的命令行来执行

```python
from mrjob.job import job


class KittyJob(MRJob):

    OUTPUT_PROTOCOL = JSONValueProtocol

    def mapper_cmd(self):
        return "grep kitty"

    def reducer(self, key, values):
        yield None, sum(1 for _ in values)


if __name__ == '__main__':
    KittyJob().run()
```


## 常用参数

`-q` 可以禁止输出debug信息

{% highlight python %}
$ python mr_most_used_word.py README.txt -q
"mrjob"
{% endhighlight %}


`> /dev/null` 可以禁止结果输出到终端


`--jobconf` hadoop运行选项

{% highlight python %}
--jobconf stream.non.zero.exit.is.failure=false
--jobconf mapreduce.job.maps=1
--jobconf mapreduce.job.reduces=1
{% endhighlight %}


`--verbose` 打印详细信息


## 问题与注意

### `lzo`文件的操作

linux下，可以用`lzop -cd x.lzo > x` 解压

{% highlight python %}
--jobconf stream.map.input.ignoreKey=true # 增加参数

# 在类中增加 HADOOP_INPUT_FORMAT
class MRIPCount(MRJob):
    HADOOP_INPUT_FORMAT = "com.hadoop.mapred.DeprecatedLzoTextInputFormat"

{% endhighlight %}

这样解析出来的 key值是行在文件中的偏移量，value值是每行的文本，所以增加ignoreKey参数

### mr引入其他包

将需要的包，进行压缩
{% highlight python %}
tar -C your-src-code -f your-src-code.tar.gz -z -c .
{% endhighlight %}

使用参数上传到hadoop cluster，并声明环境变量

{% highlight python %}
--setup 'export PYTHONPATH=$PYTHONPATH:your-src-dir.tar.gz#/'
{% endhighlight %}

注意：本机测试环境如果是python2.7的环境，hadoop集群上是python2.6环境，那么python2.7的新特性不能用，比如`from collections import Counter`

需要安装第三方以来，参考 http://pythonhosted.org/mrjob/guides/setup-cookbook.html


## 获取输入文件名称

原生的java

```java
FileSplit fileSplit = (FileSplit)reporter.getInputSplit();
String fileName = fileSplit.getPath().getName();
```

在mrjob中

```python
file_name = os.environ['mapreduce_map_input_file']

# 比如 "hdfs://hdfscluster/test/input/1.txt"
```

