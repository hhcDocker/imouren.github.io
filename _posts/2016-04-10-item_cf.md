---
layout: post
title:  "基于物品的协同过滤求视频相似"
date:   2016-04-10 14:05:31
categories: 推荐 python mrjob
tags: 推荐 python mrjob
---

* content
{:toc}



算法参考《推荐系统实践》中的item-cf算法。

具体步骤：

* 前提准备，每个视频的观看次数
* 建立用户看过的视频列表
* 记录视频被同一个用户观看的次数
* 求视频的相似度
* 汇总输出



## 前提准备

这个好写，就是求和的过程

```python

class ItemPop(MRJob):
    JOBCONF = {
        'mapreduce.job.reduces': 1
    }

    def mapper(self, _, line):
        if line:
            items = line.split()
            fudid = items[11]
            mid = items[14]
            yield mid, 1

    def combiner(self, mid, values):
        yield mid, sum(values)

    def reducer(self, mid, values):
        yield mid, sum(values)


if __name__ == '__main__':
    ItemPop.run()

```

## 计算视频的相似度

### 先求 用户 看过的视频列表

```python

def mapper1(self, _, line):
    items = line.split()
    fudid = items[11]
    mid = items[14]
    yield fudid, mid


def reducer1(self, fudid, mids):
    yield fudid, list(set(mids))

```

### 然后求，视频被同一个用户看过的次数

这里，根据视频ID做了个排序，只求一次关联数据

```python

def mapper2(self, fudid, mids):
    n = len(mids)
    for i in xrange(n):
        for j in xrange(i+1, n):
            if mids[i] >= mids[j]:
                yield (mids[i], mids[j]), 1
            else:
                yield (mids[j], mids[i]), 1

def reducer2(self, key, values):
    yield key, sum(values)

```

### 求视频的相似度。

需要用到视频被观看的次数，我们把之前算好的视频观看次数载入，路径由`self.options.item_pop`传入

```python

def configure_options(self):
    super(ItemCF, self).configure_options()

    self.add_passthrough_option(
            '--item-pop', type = 'string', default = 'hdfs:///test/item_pop',
            help='specify dictionary of item-pop data')

def load_item_pop(self):
    self.pop = {}
    fs = HadoopFilesystem(['hadoop'])
    for line in fs.cat(self.options.item_pop):
        if not line:
            continue
        key, value = line.split("\t")
        key = json.loads(key)
        value = json.loads(value)
        self.pop[key] = value

```

有了视频播放量，在reduce2的阶段就可以进行了

```python

def reducer2x(self, key, values):
    num = sum(values)
    if key[0] in self.pop and key[1] in self.pop:
        yield key[0], (key[1], num / math.sqrt(self.pop[key[0]] * self.pop[key[1]]))

```

### 最后，汇总输出到HDFS

```python

def reducer3(self, key, values):
    sim = list(values)
    sim.sort(key=lambda x: x[1], reverse=True)
    yield key, sim

```

### 执行顺序如下

```python

def steps(self):
    return [
        MRStep(
            mapper=self.mapper1,
            reducer=self.reducer1),
        MRStep(
            mapper=self.mapper2,
            reducer_init=self.load_item_pop,
            reducer=self.reducer2x),
        MRStep(
            reducer=self.reducer3,
            )
    ]

```

## 我们可以写个shell来执行这些操作

删除输入文件的脚本

```python

#!/bin/bash
SUCCESS=0
E_NOARGS=65
MYPATH="hdfs:///test/"
MYPATH_LEN=${#MYPATH}

if [ -z "$1" ]
then
    echo "Usage: `basename $0` [output_directory_path]"
    echo "`basename $0` hdfs:///test/more/deep/path"
    exit $E_NOARGS
fi

OUTPUT_PATH=$1

# if file { exists, has zero length, is a directory}
# then return 0, else return 1
snakebite test -e $OUTPUT_PATH
# hadoop fs -test -e $OUTPUT_PATH

if [ $? -eq $SUCCESS ]
then
    echo "$OUTPUT_PATH exists!"
    echo "the shell will delete $OUTPUT_PATH"
    if [ ${OUTPUT_PATH:0:$MYPATH_LEN} == $MYPATH ]
    then
        snakebite rm -R $OUTPUT_PATH
        # hadoop fs -rm -r $OUTPUT_PATH
        if [ $? -eq $SUCCESS ]
        then
            echo "$OUTPUT_PATH deleted!"
        fi
    else
        echo "no! you just can operate $MYPATH !"
    fi
else
    echo "$OUTPUT_PATH not exists! do noting ..."
fi

exit $?


```


获得输入文件

```python

DEFAULT_DAYS=10

get_input_files () {
    if [ -z "$1" ]
    then
        days=$DEFAULT_DAYS
    else
        days=$1
    fi

    all_days=""

    while [ $days -ge 1 ]
    do
        date_str=`date -d "-${days} day" +%Y/%02m/%02d`
        input="hdfs:///dw/logs/format/app_fbuffer/${date_str}/part*"
        all_days="$all_days $input"
        let "days -= 1"
    done

    echo $all_days
}

all_input=$(get_input_files $1)

```

根据执行脚本的相对路径获得其他文件的路径，并执行

```python

DATE_STR=`date -d '-1 day' +%Y/%02m/%02d`

INPUT="hdfs:///dw/logs/format/app_fbuffer/${DATE_STR}/part*"
# INPUT="hdfs:///dw/logs/format/app_fbuffer/2016/03/22/part-r-00000"
OUTPUT="hdfs:///test/item_cf/res/${DATE_STR}"

DELE_SCRIPT="delete_output_path.sh"

FILE_NAME=$(basename "$0")
FILE_PATH=$(cd "$(dirname "$0")"; pwd)
FATHER_PATH=$(dirname `dirname $FILE_PATH`)

EXTENSION="${FILE_NAME##*.}"
FILENAME="${FILE_NAME%.*}"

FULL_FILE_NAME="${FILE_PATH}/${FILENAME}.py"


CONFIG=$FATHER_PATH/config/mrjob.conf

ITEM_POP="hdfs:///test/item_pop/res/${DATE_STR}"

# 删除目标路径
${FATHER_PATH}/${DELE_SCRIPT} $OUTPUT

CMD="python $FULL_FILE_NAME -r hadoop --item-pop $ITEM_POP --conf-path=$CONFIG --no-output -o $OUTPUT $all_input --verbose "

echo $CMD

# 执行命令
time $CMD

```

这里最终执行通过HUE的ssh来进行的，需要获得hadoop的路径，脚本需要加入

```python

source /etc/profile

```
